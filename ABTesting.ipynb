{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>A Guide on A/B Testing</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Contents </h2>\n",
    "\n",
    "This project aims at giving the <b>theoretical background</b> for complete A/B testing. We do not make an attempt at coding or data engineering involved in carrying out A/B testing. This is because we believe that these days, A/B testing are carried out by existing modules in the company or by using third party A/B testing providers e.g. optimizely or google analytics.\n",
    "\n",
    "1. Introduction\n",
    "2. Statistical Background\n",
    "3. Selecting the Metrics for A/B Testing\n",
    "4. Designing the Experiment\n",
    "5. Analyzing results\n",
    "6. Bayesian approch to A/B Testing\n",
    "7. Making Profit of A/B Testing (Based on a research article in Marketing Science)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Introduction to AB Testing </h2>\n",
    "\n",
    "A/B testing is a general methodology used online when we want to test new products or features. A/B test allows us to determine scientifically how to optimize products or features (a website or an app or any system) by trying out possible changes and see what performs better with your users. It allows to make data-driven decisions rather than relying on intution about user preferences. To keep the discussion concise, hereafter, we will explain A/B testing as a method employed to determine if a new feature F2 should replace existing feature F1. This feature could be a small change (e.g. changing the font or color of a button on your website) or a complicated change (using deep learning for product recommendation in amazon). This feature could not even be user visible (changing the ranking system of friend suggestion in facebook).  \n",
    "\n",
    "Broadly speaking, A/B testing uses two sets of users. First set, the control group, is shown Feature F1 and the second set, treatment group/ experiment group is shown the new feature F2. A/B testing uses rigorous post experiment analysis to  to determine which feature performs better with users. A/B testing is generally used when we compare two options. If more than 2 options (of features) are tested, it is called multi-variate A/B testing (or split test). A brief explanation of A/B testing is shown in Figure 1 where a company wants to increase its user engagement.\n",
    "\n",
    "<img src=\"figures/AB.jpg\" width=\"800\">\n",
    "<center>Figure 1. An example of A/B testing and split test</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>History of A/B testing</h3>\n",
    "\n",
    "A/B testing is a sort-of modern day term after computers became mainstream. Such analysis have been carried out since ages. Random experiments were used by farmers for identifying the most productive variety of crop. They are similar to clinical trials in the field of medicines. A/B test is not the only option for selection of better features (or creating a best feature set incrementally). There are other techniques e.g. surveys, interviews,focus groups. They can provide more deeper qualitative data (higher resolution) than A/B tests as we know much more about the user in these techniques. However, A/B tests can easily provide data from more number of users, more diverse group of users and are less expensive to conduct over large scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>When NOT to do A/B test</h3>\n",
    "\n",
    "While A/B testing is a very handy tool to have, it may not be well suited for all types of changes or changes that are difficult to measure. A/B testing is most effective when we have a small set of changes and features to test. If we change a large number of features in a website, we would not be able to identify the reason for increase/decrease in user engagement, i.e., which feature change contributes the most for that increase/decrease. The changes should be incremental and not drastic. For example, we cannot change the outlook of a website completely. Worst case, this may lead the users in the treatment group to uninstall your app. Also, the expected change should be observed immedieately (A/B testing is not good for features that take a long time before changes are observed). For example, if we want to measure car sales, users take 2-3 months to select and buy a car. Running an A/B test for 1 week might turn useless. A/B testing is not reliable if randomization in the experiment cannot be achieved (we cannot give life saving drugs to patients randomly). It is said that we cannot establish casuality (effect of change in a feature) from observational studies, we need completely random experiments to establish the effect of change in the feature.\n",
    "\n",
    "Metric, which we discuss later in detail, also determine if A/B testing can be conducted.  If our metric is <i>how many times a user returns to our electricity bill payment website </i>, it might not be a good test if run for 1 week, given a customer will only return after 1 month. Or, if we measure, <i>the number of user referrals</i> in a mobile app, we dont know if the users would refer in 1 day or 1 month, so referrals might not be the best metrics for A/B tests.\n",
    "\n",
    "The duration of A/B testing also determines if A/B testing should be done. Too short a duration gives unreliable results while running it too long could be expensive. Also, if we run A/B testing for long, users might \"learn\" the changes in feature. This will make the test unreliable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Statistical Background </h2>\n",
    "\n",
    "Here we discuss the minimum statistics we need to know for getting started with A/B testing. There is no upper limit on the advantages of knowing more statistics. A/B testing needs rigorous post experiment analysis before implementing a feature change to ensure that we are confident on the results (and the observations were not just by chance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Randomization</h3>\n",
    "\n",
    "A/B testing is similar to causal analysis. It aims to identify if the changes in user response is due to the new feature. For example, if an existing feature F1 is the baseline, is there an increase in user engagement when users see new feature F2. Randomization is the most important assumption in field experiments. When dividing the users into two groups (control and treatment), it should be done completely random and both groups should be representative of the user base. Once you decide to run the experiment and assign users to control and treatment group, check that the user characteristics are similar (e.g. both groups should have similar ratio of males to females, age distribution, income). If the experiment is completely randomized, there is a high probability that these characteristics would be similar in both groups. \n",
    "\n",
    "There are different techniques were randomization cannot be achieved. One such method is Quasi-experimentation. For example, ride sharing services want to check a policy and they can compare it in two different cities. Another technique is controlling for the different variables in control and treatment group. For example, keeping a track if the user is employed or not, or age of the user. These helps in isolating the effect of feature F2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Confidence Interval </h3>\n",
    "\n",
    "There could be a large variety of metrics that could be tested (depending on the company requirement). These metrics could follow different distributions. In this project, we consider metrics that follow binomial or normal distribution. \n",
    "\n",
    "Binomial distribution is used to model dichotomous outcomes (success/failures, 0/1). Thus, it is characterized by the probability of success (e.g. what is the probability that a user clicks on the sing up button when they visit the page). Normal distribution is used to model continuous outcomes. It is characterized by its mean and variance.\n",
    "\n",
    "First, a <b>difference between standard deviation and standard error</b>. We can never collect the data from the whole population, so we estimate using samples of data drawn from the same population. Standard deviation is the measure of spread around the mean of the data if the data is normally distributed. It is used as a measure of dispersion for non-normally distributed data, however, it is still a valid measure of variability (spread and dispersion). If we collect a sample of data, we can calculate the spread of data by measuring its standard deviation. Similarly, if we collect data from many samples, we can observe different mean and different spread for each sample. Also, we will observe different means (from different samples) and the mean itself follow a distribution. The spread (standard deviation) of these means (centered around mean of means) is called standard error. Standard error can be estimated from a simple sample by calculating its standard deviation and dividing it by the square root of the sample size. Thus, standard error is always smaller than standard error.\n",
    "\n",
    "Confidence interval contains the two extremes, equi-distant from the center. In binomial distribution, proportion of success in the sample (e.g. number of users who click the button out of total number of visitors) is the center and in normal distribution, mean of the sample is the center. Distance from center to the boundary of the confidence interval is called margin or error. Margin of error is calculated by multiplying z-score of confidence level and the standard error. And the confidence interval is calculated by adding and subtracting the margin of error to and from the center. \n",
    "\n",
    "A 95% confidence interval can be defined as: <i>if we collect samples from the population over and over again and construct the 95% confidence intervals around the mean of these samples, 95% of the time, we would expect the true value of the population to be covered in these intervals.</i>\n",
    "\n",
    "<img src=\"figures/Confidence_interval.jpg\" width=\"800\">\n",
    "<center>Figure 2. Confidence interval and its concepts</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Sample size calculation </h3>\n",
    "\n",
    "How long the A/B tests should be continued? A/B testing should be done for metrics that can be observed in short time. However, the sample size (or data collected from A/B testing) should be enough for us to confidently accept/reject the results from A/B testing. First we discuss <b>p-value</b> and <b>significance levels</b>.\n",
    "\n",
    "In hypothesis testing, null hypothesis states the world we believe exists. Null hypothesis is the baseline in A/B testing (e.g. No difference in user engagement between control group and treatment group). Alternative hypothesis looks into our interest of difference (e.g. user engagement increases or not by using F2). <b>p-value</b> is the probability that the world we believe exists. For example, if p-value is 0.02, it says, there is a 2% chance that the world of null hypothesis exists. if p-value is smaller than the level of significance, we reject the null hypothesis, assuming we observed it by chance. Thus, <b>significance level</b> is the probability of rejecting the null hypothesis when it is true. Generally the level of significance is kept at 5%.\n",
    "\n",
    "In A/B testing, we compare two samples. We expect the two samples to behave differently (if we expect the feature to have an impact). Thus the standard error would be different for the two samples. Thus we use pooled standard error (<i>p-SE</i>). Let <i>d</i> be the difference (of probability of click) between the two groups. If our null hypothesis says there is no difference between the two groups, we expect <i>d = 0</i>. We can calculate the confidence interval, centered around 0 with margin of error by using <i>p-SE</i>. If <i>d</i> lies on either side of the extreme of the confidence interval (smaller than left side or larger than right side), we reject the null hypothesis as unlikely and say the difference between the two samples is <b>statistically significant</b>.\n",
    "\n",
    "In hypothesis testing, we might fall into two errors. In <b>Type-I</b> error, or alpha, we reject null hypothesis given it is true. In <b>Type-II</b> error, or beta, we fail to reject the null hypothesis, given it is false. They are also called producer's risk and consumer's risk respectively. Type-II error can be more serious (e.g. testing that a drug works and giving it to patients given that the medicine doesn't actually work). The power of test is defined as 1-beta, that is the probability to reject the null hypothesis when it is false. Hypothesis testing is based on the assumption that we know how many samples were needed to be collected to detect the change we wanted to see. But in practice, it is other way round, that is, we first collect data and then do the hypothesis testing. \n",
    "\n",
    "Sample size depends on the desired power of the test and desired level of change we want to detect in the two groups (control and treatment). This level of change we want to detect is called minimum detectable effect (MDE). For higher power of the test, we need larger sample size. The smaller the change we want to detect, the larger sample size we need. For example, in medicine, we might want to detect if the difference is 5% - 10%. In click through rate on advertisement, we might want to detect even a 2% change. The change we want to detect is called practically significant change. Thus, A/B testing results should detect practically and also statistically significant changes. The relation between these is explained below in Figure 3.\n",
    "\n",
    "Left part of Figure 3 shows what does power of a test graphically. As the sample size increases, the standard error decreases, thus the distribution become tighter and tighter. As seen from the figure, the chances of wrongfully concluding that no difference exists (even though difference exists) decreases. Also, as the distributions move away from each other (if there is larger difference), the power of the test is high as it can easily detect the change. Thus, it becomes difficult to detect smaller changes. Thus power increases on increasing the sample size. Power is kept at 80% generally.\n",
    "\n",
    "Check the online calculator (by optimizely) used for calculating the sample size\n",
    "https://www.optimizely.com/sample-size-calculator/\n",
    "\n",
    "<img src=\"figures/Power.jpg\" width=\"900\">\n",
    "<center>Figure 3. Calculating the Power of a test</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Selecting the Metrics for A/B Testing </h2>\n",
    "\n",
    "With the minimum statistical knowledge required for A/B testing, we begin the first step in designing A/B testing. It is very critical to understand how we select our metrics for the test as it will define the success of the A/B tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Business Sense</h3>\n",
    "\n",
    "The most important property of the metrics is that it should satisfy business needs and have some values to the company. It could be economic value e.g. increase in user clicks or it could also be done for understanding user behavior. The managers should know what are they going to  use the metrics for. The metrics should be well defined, that is, there should be no confusion on what formula is to be used for calculating the metric (click through rate is different from number of clicks/views). The developers should be able to measure the metrics in short time. For example, sign up does not tell if the the user is going to refer or not. As mentioned before, the metrics should not take long time to measure (e.g. effect of a soap on skin cancer). The company should have complete information about the metric. We should be able to measure the metric (is it mean, median) and understand possible error in collecting the metrics (e.g. if click through rate increase by 30%, there might be a bug in the code in collecting the data). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Filtering</h3>\n",
    "\n",
    "We should be able to identify issues while collecting these metrics. For example, we should check that there is no spam that is affecting our data collection and we should be able to filter these. Users in control and treatment group should not know if they have been placed in one of the groups (it affects the randomization of the experiment and users might self-select to either join or leave the experiment). Also, users should not know that an experiment is being carried out. This might affect the metrics as users in control group would want to test the new feature if they know about it and this might pollute the current experiment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Invariance check</h3>\n",
    "\n",
    "Before running the experiment and analyzing the results, A/B testing should go through sanity checks for randomization. The distribution of user characteristics in control and treatment groups should be the same. This can also be done by A/A testing. That is instead of sending the new feature F2 to the treatment group, just send feature F1. As the user groups are randomly created, there should be no difference in user engagement between both the groups.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Sensitivity and Robustness</h3>\n",
    "\n",
    "The metrics should be sensitive enough to move if there is an actual difference in the response in control and the treatment group. but it should not be very sensitive that a slight change would result in a very different measures. For example, if our metric is mean of total donations, mean is sensitive to outliers. if our metric is median, it is robust to outliers but it might be very insensitive. For example, if just 1% of users buy a product, and we use median, the median might not move and it will 0 in both the groups.\n",
    "\n",
    "Before selecting the metrics, a histogram of the metric for the existing users should be used to identify the nature of metrics. Different metrics include mean, median, percentiles, ratios, or percent change. Percent change is insensitive to changes in absolute values (which is desirable in many situations). A/A test might also be able to test too sensitive metrics as the metrics will be different only because the users are different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Variability</h3>\n",
    "\n",
    "Variability or variance is critical for analyzing the results of A/B testing as we use confidence intervals and hypothesis testing for detecting the change. Metrics may follow different distributions. For calculating the confidence interval and analyzing the results, we should know about the distribution of the metrics and how to calculate the variability. Estimating variance may not always be easy (e.g. median of a non-normal distribution or a complex distribution) when analytical result for variance calculation is not possible.\n",
    "\n",
    "If the variance cannot be calculated variance analytically, we can calculate it empirically. Variability calculation makes an underlying assumption that we know the inherent distribution of the metric. We it is complex, we can run a lot of small A/A tests to identify the underlying distribution of the metrics. Or we can do one big A/A test and use techniques like bootstrapping (sampling with replacement) to get the underlying distribution of the metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Designing the Experiment </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Analyzing Results </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Multi-variate A/B testing</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Bayesian Approach to A/B Testing </h2>\n",
    "\n",
    "Before going through the Bayesian A/B testing, it is important to understand bayes theorem, and definition of prior, likelihood and posterior. Please go through the following video: https://www.youtube.com/watch?v=HZGCoVF3YvM before starting to read the bayesian part of A/B testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Frequentist vs Bayesian in A/B testing</h3>\n",
    "\n",
    "We discussed frequentist approach for A/B testing. Frequentist approach uses p-value to determine if feature F2 performs better than feature F1. If the difference between the control and treatment group is statistically significant (based on p-values), or if enough data has been collected and it is still statistically not different, A/B testing is stopped. There is no uncertainty in the frequentist approach (when we stop, the results say if we should go go the changes or not). A/B testing is believed to be biased for \"baseline/ keeping as is\". After A/B testing, we have to decide if the difference is (statistically and practically) significant so that it can be implemented (there is a cost in deploying the new features/ products/ upgrade to the system).\n",
    "\n",
    "Bayesian A/B Testing employs Bayesian inference methods to give you ‘probability’ of how much A is better (or worse) than B. A frequentist approach cannot provide this measure. This makes it more intuitive and slighly easier to convey as compared to frequentist approach which requires some knowledge of statistics. Also, inference for A/B testing can be updated continuously as new data is observed. Thus, we do not have to wait for certain amount of data and its analysis to decide if we want to stop or continue with the experiment (Bayesian A/B testing also performs better with more data). Bayesian approach embraces uncertainty. It tells us how certain are we that F2 is better than F1. Frequentist approach uses 95% confidence levels (reject if p-value < 5%). Why these numbers? It should be case based, why so arbritary? These p-values keep changing (as we collect more data). p-Values may sometimes go below 5% and it would be a mistake to stop our experiments there as it just came down for that one instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Steps for Bayesian A/B testing</h3>\n",
    "\n",
    "Using A/A testing or using the domain knowledge, first a prior is built on the metrics considered for the experiment (e.g. managers might have a good idea of clickthrough rate or donations collected). Using the distribution from which prior is built, appropriate prior distribution can b selected. Life will be so much easier if we can use a conjugate prior to the distribution we expect to be followed by the metrics. For example, if we model clickthrough rate as binomial distribution, we can use beta distribution as its prior. \n",
    "\n",
    "We know from bayes theorem that posterior depends on the relative strength of prior and likelihood. If we are confident about our prior, that is, we are sure that the metrics follow a certain distribution with certain parameters (probability of clickthrough ranges from 0.1 to 0.15, or the 25th and 75th percentile of donation value is \\\\$30 and \\\\$60), we can use a strong prior. If we are not so sure, we can use a weak prior. Strength of likelihood increases as more and more data is collected. Thus, generally a we start with a weak prior and get the posterior in an online manner as more and more data is observed (and posterior converges over time as more data is collected). \n",
    "\n",
    "An example of strength of prior is shown using beta distribution. Please note that the mean (given by $\\frac{\\alpha}{\\beta}$ ) remains the same for the three distribution but variance changes (and hence the strength changes). Beta(4,21) is weaker than Beta (8,42) which is weaker than Beta (32,164). By weaker we mean if adding new data points, how easily the distribution can be changed. For example, if we observe two more data points where none of them was success, the posterior distribution would be Beta(4,23), Beta (8,44) and Beta (32,164). Thus the posterior for the weak prior looks very different from the prior distribution. Bayesians always face the criticism of how do we select the prior and what happens if we are not able to find a conjugate prior - that is where domain knowledge and rigorous sanity check comes in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Making Profit of A/B Testing (Based on a research article in Marketing Science)</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
